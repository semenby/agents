{"version":3,"file":"index.cjs","sources":["../../../../src/llm/google/index.ts"],"sourcesContent":["/* eslint-disable @typescript-eslint/ban-ts-comment */\nimport { AIMessageChunk } from '@langchain/core/messages';\nimport { ChatGenerationChunk } from '@langchain/core/outputs';\nimport { ChatGoogleGenerativeAI } from '@langchain/google-genai';\nimport { getEnvironmentVariable } from '@langchain/core/utils/env';\nimport { GoogleGenerativeAI as GenerativeAI } from '@google/generative-ai';\nimport type {\n  GenerateContentRequest,\n  SafetySetting,\n} from '@google/generative-ai';\nimport type { CallbackManagerForLLMRun } from '@langchain/core/callbacks/manager';\nimport type { BaseMessage, UsageMetadata } from '@langchain/core/messages';\nimport type { GeminiGenerationConfig } from '@langchain/google-common';\nimport type { GeminiApiUsageMetadata, InputTokenDetails } from './types';\nimport type { GoogleClientOptions } from '@/types';\nimport {\n  convertResponseContentToChatGenerationChunk,\n  convertBaseMessagesToContent,\n  mapGenerateContentResultToChatResult,\n} from './utils/common';\n\nexport class CustomChatGoogleGenerativeAI extends ChatGoogleGenerativeAI {\n  thinkingConfig?: GeminiGenerationConfig['thinkingConfig'];\n\n  /**\n   * Override to add gemini-3 model support for multimodal and function calling thought signatures\n   */\n  get _isMultimodalModel(): boolean {\n    return (\n      this.model.startsWith('gemini-1.5') ||\n      this.model.startsWith('gemini-2') ||\n      (this.model.startsWith('gemma-3-') &&\n        !this.model.startsWith('gemma-3-1b')) ||\n      this.model.startsWith('gemini-3')\n    );\n  }\n\n  constructor(fields: GoogleClientOptions) {\n    super(fields);\n\n    this.model = fields.model.replace(/^models\\//, '');\n\n    this.maxOutputTokens = fields.maxOutputTokens ?? this.maxOutputTokens;\n\n    if (this.maxOutputTokens != null && this.maxOutputTokens < 0) {\n      throw new Error('`maxOutputTokens` must be a positive integer');\n    }\n\n    this.temperature = fields.temperature ?? this.temperature;\n    if (\n      this.temperature != null &&\n      (this.temperature < 0 || this.temperature > 2)\n    ) {\n      throw new Error('`temperature` must be in the range of [0.0,2.0]');\n    }\n\n    this.topP = fields.topP ?? this.topP;\n    if (this.topP != null && this.topP < 0) {\n      throw new Error('`topP` must be a positive integer');\n    }\n\n    if (this.topP != null && this.topP > 1) {\n      throw new Error('`topP` must be below 1.');\n    }\n\n    this.topK = fields.topK ?? this.topK;\n    if (this.topK != null && this.topK < 0) {\n      throw new Error('`topK` must be a positive integer');\n    }\n\n    this.stopSequences = fields.stopSequences ?? this.stopSequences;\n\n    this.apiKey = fields.apiKey ?? getEnvironmentVariable('GOOGLE_API_KEY');\n    if (this.apiKey == null || this.apiKey === '') {\n      throw new Error(\n        'Please set an API key for Google GenerativeAI ' +\n          'in the environment variable GOOGLE_API_KEY ' +\n          'or in the `apiKey` field of the ' +\n          'ChatGoogleGenerativeAI constructor'\n      );\n    }\n\n    this.safetySettings = fields.safetySettings ?? this.safetySettings;\n    if (this.safetySettings && this.safetySettings.length > 0) {\n      const safetySettingsSet = new Set(\n        this.safetySettings.map((s) => s.category)\n      );\n      if (safetySettingsSet.size !== this.safetySettings.length) {\n        throw new Error(\n          'The categories in `safetySettings` array must be unique'\n        );\n      }\n    }\n\n    this.thinkingConfig = fields.thinkingConfig ?? this.thinkingConfig;\n\n    this.streaming = fields.streaming ?? this.streaming;\n    this.json = fields.json;\n\n    // @ts-ignore - Accessing private property from parent class\n    this.client = new GenerativeAI(this.apiKey).getGenerativeModel(\n      {\n        model: this.model,\n        safetySettings: this.safetySettings as SafetySetting[],\n        generationConfig: {\n          stopSequences: this.stopSequences,\n          maxOutputTokens: this.maxOutputTokens,\n          temperature: this.temperature,\n          topP: this.topP,\n          topK: this.topK,\n          ...(this.json != null\n            ? { responseMimeType: 'application/json' }\n            : {}),\n        },\n      },\n      {\n        apiVersion: fields.apiVersion,\n        baseUrl: fields.baseUrl,\n        customHeaders: fields.customHeaders,\n      }\n    );\n    this.streamUsage = fields.streamUsage ?? this.streamUsage;\n  }\n\n  static lc_name(): 'LibreChatGoogleGenerativeAI' {\n    return 'LibreChatGoogleGenerativeAI';\n  }\n\n  /**\n   * Helper function to convert Gemini API usage metadata to LangChain format\n   * Includes support for cached tokens and tier-based tracking for gemini-3-pro-preview\n   */\n  private _convertToUsageMetadata(\n    usageMetadata: GeminiApiUsageMetadata | undefined,\n    model: string\n  ): UsageMetadata | undefined {\n    if (!usageMetadata) {\n      return undefined;\n    }\n\n    const output: UsageMetadata = {\n      input_tokens: usageMetadata.promptTokenCount ?? 0,\n      output_tokens:\n        (usageMetadata.candidatesTokenCount ?? 0) +\n        (usageMetadata.thoughtsTokenCount ?? 0),\n      total_tokens: usageMetadata.totalTokenCount ?? 0,\n    };\n\n    if (usageMetadata.cachedContentTokenCount) {\n      output.input_token_details ??= {};\n      output.input_token_details.cache_read =\n        usageMetadata.cachedContentTokenCount;\n    }\n\n    // gemini-3-pro-preview has bracket based tracking of tokens per request\n    if (model === 'gemini-3-pro-preview') {\n      const over200k = Math.max(\n        0,\n        (usageMetadata.promptTokenCount ?? 0) - 200000\n      );\n      const cachedOver200k = Math.max(\n        0,\n        (usageMetadata.cachedContentTokenCount ?? 0) - 200000\n      );\n      if (over200k) {\n        output.input_token_details = {\n          ...output.input_token_details,\n          over_200k: over200k,\n        } as InputTokenDetails;\n      }\n      if (cachedOver200k) {\n        output.input_token_details = {\n          ...output.input_token_details,\n          cache_read_over_200k: cachedOver200k,\n        } as InputTokenDetails;\n      }\n    }\n\n    return output;\n  }\n\n  invocationParams(\n    options?: this['ParsedCallOptions']\n  ): Omit<GenerateContentRequest, 'contents'> {\n    const params = super.invocationParams(options);\n    if (this.thinkingConfig) {\n      /** @ts-ignore */\n      this.client.generationConfig = {\n        /** @ts-ignore */\n        ...this.client.generationConfig,\n        /** @ts-ignore */\n        thinkingConfig: this.thinkingConfig,\n      };\n    }\n    return params;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this['ParsedCallOptions'],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<import('@langchain/core/outputs').ChatResult> {\n    const prompt = convertBaseMessagesToContent(\n      messages,\n      this._isMultimodalModel,\n      this.useSystemInstruction,\n      this.model\n    );\n    let actualPrompt = prompt;\n    if (prompt?.[0].role === 'system') {\n      const [systemInstruction] = prompt;\n      /** @ts-ignore */\n      this.client.systemInstruction = systemInstruction;\n      actualPrompt = prompt.slice(1);\n    }\n    const parameters = this.invocationParams(options);\n    const request = {\n      ...parameters,\n      contents: actualPrompt,\n    };\n\n    const res = await this.caller.callWithOptions(\n      { signal: options.signal },\n      async () =>\n        /** @ts-ignore */\n        this.client.generateContent(request)\n    );\n\n    const response = res.response;\n    const usageMetadata = this._convertToUsageMetadata(\n      /** @ts-ignore */\n      response.usageMetadata,\n      this.model\n    );\n\n    /** @ts-ignore */\n    const generationResult = mapGenerateContentResultToChatResult(response, {\n      usageMetadata,\n    });\n\n    await runManager?.handleLLMNewToken(\n      generationResult.generations[0].text || '',\n      undefined,\n      undefined,\n      undefined,\n      undefined,\n      undefined\n    );\n    return generationResult;\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this['ParsedCallOptions'],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const prompt = convertBaseMessagesToContent(\n      messages,\n      this._isMultimodalModel,\n      this.useSystemInstruction,\n      this.model\n    );\n    let actualPrompt = prompt;\n    if (prompt?.[0].role === 'system') {\n      const [systemInstruction] = prompt;\n      /** @ts-ignore */\n      this.client.systemInstruction = systemInstruction;\n      actualPrompt = prompt.slice(1);\n    }\n    const parameters = this.invocationParams(options);\n    const request = {\n      ...parameters,\n      contents: actualPrompt,\n    };\n    const stream = await this.caller.callWithOptions(\n      { signal: options.signal },\n      async () => {\n        /** @ts-ignore */\n        const { stream } = await this.client.generateContentStream(request);\n        return stream;\n      }\n    );\n\n    let index = 0;\n    let lastUsageMetadata: UsageMetadata | undefined;\n    for await (const response of stream) {\n      if (\n        'usageMetadata' in response &&\n        this.streamUsage !== false &&\n        options.streamUsage !== false\n      ) {\n        lastUsageMetadata = this._convertToUsageMetadata(\n          response.usageMetadata as GeminiApiUsageMetadata | undefined,\n          this.model\n        );\n      }\n\n      const chunk = convertResponseContentToChatGenerationChunk(response, {\n        usageMetadata: undefined,\n        index,\n      });\n      index += 1;\n      if (!chunk) {\n        continue;\n      }\n\n      yield chunk;\n      await runManager?.handleLLMNewToken(\n        chunk.text || '',\n        undefined,\n        undefined,\n        undefined,\n        undefined,\n        { chunk }\n      );\n    }\n\n    if (lastUsageMetadata) {\n      const finalChunk = new ChatGenerationChunk({\n        text: '',\n        message: new AIMessageChunk({\n          content: '',\n          usage_metadata: lastUsageMetadata,\n        }),\n      });\n      yield finalChunk;\n      await runManager?.handleLLMNewToken(\n        finalChunk.text || '',\n        undefined,\n        undefined,\n        undefined,\n        undefined,\n        { chunk: finalChunk }\n      );\n    }\n  }\n}\n"],"names":["ChatGoogleGenerativeAI","getEnvironmentVariable","GenerativeAI","convertBaseMessagesToContent","mapGenerateContentResultToChatResult","messages","convertResponseContentToChatGenerationChunk","ChatGenerationChunk","AIMessageChunk"],"mappings":";;;;;;;;;AAAA;AAqBM,MAAO,4BAA6B,SAAQA,kCAAsB,CAAA;AACtE,IAAA,cAAc;AAEd;;AAEG;AACH,IAAA,IAAI,kBAAkB,GAAA;QACpB,QACE,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,YAAY,CAAC;AACnC,YAAA,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,UAAU,CAAC;AACjC,aAAC,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,UAAU,CAAC;gBAChC,CAAC,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,YAAY,CAAC,CAAC;YACvC,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,UAAU,CAAC;;AAIrC,IAAA,WAAA,CAAY,MAA2B,EAAA;QACrC,KAAK,CAAC,MAAM,CAAC;AAEb,QAAA,IAAI,CAAC,KAAK,GAAG,MAAM,CAAC,KAAK,CAAC,OAAO,CAAC,WAAW,EAAE,EAAE,CAAC;QAElD,IAAI,CAAC,eAAe,GAAG,MAAM,CAAC,eAAe,IAAI,IAAI,CAAC,eAAe;AAErE,QAAA,IAAI,IAAI,CAAC,eAAe,IAAI,IAAI,IAAI,IAAI,CAAC,eAAe,GAAG,CAAC,EAAE;AAC5D,YAAA,MAAM,IAAI,KAAK,CAAC,8CAA8C,CAAC;;QAGjE,IAAI,CAAC,WAAW,GAAG,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,WAAW;AACzD,QAAA,IACE,IAAI,CAAC,WAAW,IAAI,IAAI;AACxB,aAAC,IAAI,CAAC,WAAW,GAAG,CAAC,IAAI,IAAI,CAAC,WAAW,GAAG,CAAC,CAAC,EAC9C;AACA,YAAA,MAAM,IAAI,KAAK,CAAC,iDAAiD,CAAC;;QAGpE,IAAI,CAAC,IAAI,GAAG,MAAM,CAAC,IAAI,IAAI,IAAI,CAAC,IAAI;AACpC,QAAA,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,EAAE;AACtC,YAAA,MAAM,IAAI,KAAK,CAAC,mCAAmC,CAAC;;AAGtD,QAAA,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,EAAE;AACtC,YAAA,MAAM,IAAI,KAAK,CAAC,yBAAyB,CAAC;;QAG5C,IAAI,CAAC,IAAI,GAAG,MAAM,CAAC,IAAI,IAAI,IAAI,CAAC,IAAI;AACpC,QAAA,IAAI,IAAI,CAAC,IAAI,IAAI,IAAI,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,EAAE;AACtC,YAAA,MAAM,IAAI,KAAK,CAAC,mCAAmC,CAAC;;QAGtD,IAAI,CAAC,aAAa,GAAG,MAAM,CAAC,aAAa,IAAI,IAAI,CAAC,aAAa;QAE/D,IAAI,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,IAAIC,0BAAsB,CAAC,gBAAgB,CAAC;AACvE,QAAA,IAAI,IAAI,CAAC,MAAM,IAAI,IAAI,IAAI,IAAI,CAAC,MAAM,KAAK,EAAE,EAAE;YAC7C,MAAM,IAAI,KAAK,CACb,gDAAgD;gBAC9C,6CAA6C;gBAC7C,kCAAkC;AAClC,gBAAA,oCAAoC,CACvC;;QAGH,IAAI,CAAC,cAAc,GAAG,MAAM,CAAC,cAAc,IAAI,IAAI,CAAC,cAAc;AAClE,QAAA,IAAI,IAAI,CAAC,cAAc,IAAI,IAAI,CAAC,cAAc,CAAC,MAAM,GAAG,CAAC,EAAE;YACzD,MAAM,iBAAiB,GAAG,IAAI,GAAG,CAC/B,IAAI,CAAC,cAAc,CAAC,GAAG,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,CAC3C;YACD,IAAI,iBAAiB,CAAC,IAAI,KAAK,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE;AACzD,gBAAA,MAAM,IAAI,KAAK,CACb,yDAAyD,CAC1D;;;QAIL,IAAI,CAAC,cAAc,GAAG,MAAM,CAAC,cAAc,IAAI,IAAI,CAAC,cAAc;QAElE,IAAI,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,IAAI,IAAI,CAAC,SAAS;AACnD,QAAA,IAAI,CAAC,IAAI,GAAG,MAAM,CAAC,IAAI;;AAGvB,QAAA,IAAI,CAAC,MAAM,GAAG,IAAIC,+BAAY,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,kBAAkB,CAC5D;YACE,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,cAAc,EAAE,IAAI,CAAC,cAAiC;AACtD,YAAA,gBAAgB,EAAE;gBAChB,aAAa,EAAE,IAAI,CAAC,aAAa;gBACjC,eAAe,EAAE,IAAI,CAAC,eAAe;gBACrC,WAAW,EAAE,IAAI,CAAC,WAAW;gBAC7B,IAAI,EAAE,IAAI,CAAC,IAAI;gBACf,IAAI,EAAE,IAAI,CAAC,IAAI;AACf,gBAAA,IAAI,IAAI,CAAC,IAAI,IAAI;AACf,sBAAE,EAAE,gBAAgB,EAAE,kBAAkB;sBACtC,EAAE,CAAC;AACR,aAAA;SACF,EACD;YACE,UAAU,EAAE,MAAM,CAAC,UAAU;YAC7B,OAAO,EAAE,MAAM,CAAC,OAAO;YACvB,aAAa,EAAE,MAAM,CAAC,aAAa;AACpC,SAAA,CACF;QACD,IAAI,CAAC,WAAW,GAAG,MAAM,CAAC,WAAW,IAAI,IAAI,CAAC,WAAW;;AAG3D,IAAA,OAAO,OAAO,GAAA;AACZ,QAAA,OAAO,6BAA6B;;AAGtC;;;AAGG;IACK,uBAAuB,CAC7B,aAAiD,EACjD,KAAa,EAAA;QAEb,IAAI,CAAC,aAAa,EAAE;AAClB,YAAA,OAAO,SAAS;;AAGlB,QAAA,MAAM,MAAM,GAAkB;AAC5B,YAAA,YAAY,EAAE,aAAa,CAAC,gBAAgB,IAAI,CAAC;AACjD,YAAA,aAAa,EACX,CAAC,aAAa,CAAC,oBAAoB,IAAI,CAAC;AACxC,iBAAC,aAAa,CAAC,kBAAkB,IAAI,CAAC,CAAC;AACzC,YAAA,YAAY,EAAE,aAAa,CAAC,eAAe,IAAI,CAAC;SACjD;AAED,QAAA,IAAI,aAAa,CAAC,uBAAuB,EAAE;AACzC,YAAA,MAAM,CAAC,mBAAmB,KAAK,EAAE;YACjC,MAAM,CAAC,mBAAmB,CAAC,UAAU;gBACnC,aAAa,CAAC,uBAAuB;;;AAIzC,QAAA,IAAI,KAAK,KAAK,sBAAsB,EAAE;AACpC,YAAA,MAAM,QAAQ,GAAG,IAAI,CAAC,GAAG,CACvB,CAAC,EACD,CAAC,aAAa,CAAC,gBAAgB,IAAI,CAAC,IAAI,MAAM,CAC/C;AACD,YAAA,MAAM,cAAc,GAAG,IAAI,CAAC,GAAG,CAC7B,CAAC,EACD,CAAC,aAAa,CAAC,uBAAuB,IAAI,CAAC,IAAI,MAAM,CACtD;YACD,IAAI,QAAQ,EAAE;gBACZ,MAAM,CAAC,mBAAmB,GAAG;oBAC3B,GAAG,MAAM,CAAC,mBAAmB;AAC7B,oBAAA,SAAS,EAAE,QAAQ;iBACC;;YAExB,IAAI,cAAc,EAAE;gBAClB,MAAM,CAAC,mBAAmB,GAAG;oBAC3B,GAAG,MAAM,CAAC,mBAAmB;AAC7B,oBAAA,oBAAoB,EAAE,cAAc;iBAChB;;;AAI1B,QAAA,OAAO,MAAM;;AAGf,IAAA,gBAAgB,CACd,OAAmC,EAAA;QAEnC,MAAM,MAAM,GAAG,KAAK,CAAC,gBAAgB,CAAC,OAAO,CAAC;AAC9C,QAAA,IAAI,IAAI,CAAC,cAAc,EAAE;;AAEvB,YAAA,IAAI,CAAC,MAAM,CAAC,gBAAgB,GAAG;;AAE7B,gBAAA,GAAG,IAAI,CAAC,MAAM,CAAC,gBAAgB;;gBAE/B,cAAc,EAAE,IAAI,CAAC,cAAc;aACpC;;AAEH,QAAA,OAAO,MAAM;;AAGf,IAAA,MAAM,SAAS,CACb,QAAuB,EACvB,OAAkC,EAClC,UAAqC,EAAA;AAErC,QAAA,MAAM,MAAM,GAAGC,mCAA4B,CACzC,QAAQ,EACR,IAAI,CAAC,kBAAkB,EACvB,IAAI,CAAC,oBAAoB,EACzB,IAAI,CAAC,KAAK,CACX;QACD,IAAI,YAAY,GAAG,MAAM;QACzB,IAAI,MAAM,GAAG,CAAC,CAAC,CAAC,IAAI,KAAK,QAAQ,EAAE;AACjC,YAAA,MAAM,CAAC,iBAAiB,CAAC,GAAG,MAAM;;AAElC,YAAA,IAAI,CAAC,MAAM,CAAC,iBAAiB,GAAG,iBAAiB;AACjD,YAAA,YAAY,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;;QAEhC,MAAM,UAAU,GAAG,IAAI,CAAC,gBAAgB,CAAC,OAAO,CAAC;AACjD,QAAA,MAAM,OAAO,GAAG;AACd,YAAA,GAAG,UAAU;AACb,YAAA,QAAQ,EAAE,YAAY;SACvB;AAED,QAAA,MAAM,GAAG,GAAG,MAAM,IAAI,CAAC,MAAM,CAAC,eAAe,CAC3C,EAAE,MAAM,EAAE,OAAO,CAAC,MAAM,EAAE,EAC1B;;QAEE,IAAI,CAAC,MAAM,CAAC,eAAe,CAAC,OAAO,CAAC,CACvC;AAED,QAAA,MAAM,QAAQ,GAAG,GAAG,CAAC,QAAQ;AAC7B,QAAA,MAAM,aAAa,GAAG,IAAI,CAAC,uBAAuB;;AAEhD,QAAA,QAAQ,CAAC,aAAa,EACtB,IAAI,CAAC,KAAK,CACX;;AAGD,QAAA,MAAM,gBAAgB,GAAGC,2CAAoC,CAAC,QAAQ,EAAE;YACtE,aAAa;AACd,SAAA,CAAC;QAEF,MAAM,UAAU,EAAE,iBAAiB,CACjC,gBAAgB,CAAC,WAAW,CAAC,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE,EAC1C,SAAS,EACT,SAAS,EACT,SAAS,EACT,SAAS,EACT,SAAS,CACV;AACD,QAAA,OAAO,gBAAgB;;IAGzB,OAAO,qBAAqB,CAC1BC,UAAuB,EACvB,OAAkC,EAClC,UAAqC,EAAA;AAErC,QAAA,MAAM,MAAM,GAAGF,mCAA4B,CACzCE,UAAQ,EACR,IAAI,CAAC,kBAAkB,EACvB,IAAI,CAAC,oBAAoB,EACzB,IAAI,CAAC,KAAK,CACX;QACD,IAAI,YAAY,GAAG,MAAM;QACzB,IAAI,MAAM,GAAG,CAAC,CAAC,CAAC,IAAI,KAAK,QAAQ,EAAE;AACjC,YAAA,MAAM,CAAC,iBAAiB,CAAC,GAAG,MAAM;;AAElC,YAAA,IAAI,CAAC,MAAM,CAAC,iBAAiB,GAAG,iBAAiB;AACjD,YAAA,YAAY,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;;QAEhC,MAAM,UAAU,GAAG,IAAI,CAAC,gBAAgB,CAAC,OAAO,CAAC;AACjD,QAAA,MAAM,OAAO,GAAG;AACd,YAAA,GAAG,UAAU;AACb,YAAA,QAAQ,EAAE,YAAY;SACvB;AACD,QAAA,MAAM,MAAM,GAAG,MAAM,IAAI,CAAC,MAAM,CAAC,eAAe,CAC9C,EAAE,MAAM,EAAE,OAAO,CAAC,MAAM,EAAE,EAC1B,YAAW;;AAET,YAAA,MAAM,EAAE,MAAM,EAAE,GAAG,MAAM,IAAI,CAAC,MAAM,CAAC,qBAAqB,CAAC,OAAO,CAAC;AACnE,YAAA,OAAO,MAAM;AACf,SAAC,CACF;AAGD,QAAA,IAAI,iBAA4C;AAChD,QAAA,WAAW,MAAM,QAAQ,IAAI,MAAM,EAAE;YACnC,IACE,eAAe,IAAI,QAAQ;gBAC3B,IAAI,CAAC,WAAW,KAAK,KAAK;AAC1B,gBAAA,OAAO,CAAC,WAAW,KAAK,KAAK,EAC7B;AACA,gBAAA,iBAAiB,GAAG,IAAI,CAAC,uBAAuB,CAC9C,QAAQ,CAAC,aAAmD,EAC5D,IAAI,CAAC,KAAK,CACX;;AAGH,YAAA,MAAM,KAAK,GAAGC,kDAA2C,CAAC,QAAQ,EAAE;AAClE,gBAAA,aAAa,EAAE,SAEhB,CAAA,CAAC;YAEF,IAAI,CAAC,KAAK,EAAE;gBACV;;AAGF,YAAA,MAAM,KAAK;YACX,MAAM,UAAU,EAAE,iBAAiB,CACjC,KAAK,CAAC,IAAI,IAAI,EAAE,EAChB,SAAS,EACT,SAAS,EACT,SAAS,EACT,SAAS,EACT,EAAE,KAAK,EAAE,CACV;;QAGH,IAAI,iBAAiB,EAAE;AACrB,YAAA,MAAM,UAAU,GAAG,IAAIC,2BAAmB,CAAC;AACzC,gBAAA,IAAI,EAAE,EAAE;gBACR,OAAO,EAAE,IAAIC,uBAAc,CAAC;AAC1B,oBAAA,OAAO,EAAE,EAAE;AACX,oBAAA,cAAc,EAAE,iBAAiB;iBAClC,CAAC;AACH,aAAA,CAAC;AACF,YAAA,MAAM,UAAU;YAChB,MAAM,UAAU,EAAE,iBAAiB,CACjC,UAAU,CAAC,IAAI,IAAI,EAAE,EACrB,SAAS,EACT,SAAS,EACT,SAAS,EACT,SAAS,EACT,EAAE,KAAK,EAAE,UAAU,EAAE,CACtB;;;AAGN;;;;"}