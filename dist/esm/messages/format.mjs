import { HumanMessage, AIMessage, SystemMessage, getBufferString, ToolMessage, AIMessageChunk } from '@langchain/core/messages';
import { Providers, ContentTypes } from '../common/enum.mjs';

/* eslint-disable @typescript-eslint/no-explicit-any */
/**
 * Formats a message with media content (images, documents, videos, audios) to API payload format.
 *
 * @param params - The parameters for formatting.
 * @returns - The formatted message.
 */
const formatMediaMessage = ({ message, endpoint, mediaParts, }) => {
    // Create a new object to avoid mutating the input
    const result = {
        ...message,
        content: [],
    };
    if (endpoint === Providers.ANTHROPIC) {
        result.content = [
            ...mediaParts,
            { type: ContentTypes.TEXT, text: message.content },
        ];
        return result;
    }
    result.content = [
        { type: ContentTypes.TEXT, text: message.content },
        ...mediaParts,
    ];
    return result;
};
/**
 * Formats a message to OpenAI payload format based on the provided options.
 *
 * @param params - The parameters for formatting.
 * @returns - The formatted message.
 */
const formatMessage = ({ message, userName, endpoint, assistantName, langChain = false, }) => {
    // eslint-disable-next-line prefer-const
    let { role: _role, _name, sender, text, content: _content, lc_id } = message;
    if (lc_id && lc_id[2] && !langChain) {
        const roleMapping = {
            SystemMessage: 'system',
            HumanMessage: 'user',
            AIMessage: 'assistant',
        };
        _role = roleMapping[lc_id[2]] || _role;
    }
    const role = _role ??
        (sender != null && sender && sender.toLowerCase() === 'user'
            ? 'user'
            : 'assistant');
    const content = _content ?? text ?? '';
    const formattedMessage = {
        role,
        content,
    };
    // Set name fields first
    if (_name != null && _name) {
        formattedMessage.name = _name;
    }
    if (userName != null && userName && formattedMessage.role === 'user') {
        formattedMessage.name = userName;
    }
    if (assistantName != null &&
        assistantName &&
        formattedMessage.role === 'assistant') {
        formattedMessage.name = assistantName;
    }
    if (formattedMessage.name != null && formattedMessage.name) {
        // Conform to API regex: ^[a-zA-Z0-9_-]{1,64}$
        // https://community.openai.com/t/the-format-of-the-name-field-in-the-documentation-is-incorrect/175684/2
        formattedMessage.name = formattedMessage.name.replace(/[^a-zA-Z0-9_-]/g, '_');
        if (formattedMessage.name.length > 64) {
            formattedMessage.name = formattedMessage.name.substring(0, 64);
        }
    }
    const { image_urls, documents, videos, audios } = message;
    const mediaParts = [];
    if (Array.isArray(documents) && documents.length > 0) {
        mediaParts.push(...documents);
    }
    if (Array.isArray(videos) && videos.length > 0) {
        mediaParts.push(...videos);
    }
    if (Array.isArray(audios) && audios.length > 0) {
        mediaParts.push(...audios);
    }
    if (Array.isArray(image_urls) && image_urls.length > 0) {
        mediaParts.push(...image_urls);
    }
    if (mediaParts.length > 0 && role === 'user') {
        const mediaMessage = formatMediaMessage({
            message: {
                ...formattedMessage,
                content: typeof formattedMessage.content === 'string'
                    ? formattedMessage.content
                    : '',
            },
            mediaParts,
            endpoint,
        });
        if (!langChain) {
            return mediaMessage;
        }
        return new HumanMessage(mediaMessage);
    }
    if (!langChain) {
        return formattedMessage;
    }
    if (role === 'user') {
        return new HumanMessage(formattedMessage);
    }
    else if (role === 'assistant') {
        return new AIMessage(formattedMessage);
    }
    else {
        return new SystemMessage(formattedMessage);
    }
};
/**
 * Formats an array of messages for LangChain.
 *
 * @param messages - The array of messages to format.
 * @param formatOptions - The options for formatting each message.
 * @returns - The array of formatted LangChain messages.
 */
const formatLangChainMessages = (messages, formatOptions) => {
    return messages.map((msg) => {
        const formatted = formatMessage({
            ...formatOptions,
            message: msg,
            langChain: true,
        });
        return formatted;
    });
};
/**
 * Formats a LangChain message object by merging properties from `lc_kwargs` or `kwargs` and `additional_kwargs`.
 *
 * @param message - The message object to format.
 * @returns - The formatted LangChain message.
 */
const formatFromLangChain = (message) => {
    const kwargs = message.lc_kwargs ?? message.kwargs ?? {};
    const { additional_kwargs = {}, ...message_kwargs } = kwargs;
    return {
        ...message_kwargs,
        ...additional_kwargs,
    };
};
/**
 * Helper function to format an assistant message
 * @param message The message to format
 * @returns Array of formatted messages
 */
function formatAssistantMessage(message) {
    const formattedMessages = [];
    let currentContent = [];
    let lastAIMessage = null;
    let hasReasoning = false;
    if (Array.isArray(message.content)) {
        for (const part of message.content) {
            if (part.type === ContentTypes.TEXT && part.tool_call_ids) {
                /*
                If there's pending content, it needs to be aggregated as a single string to prepare for tool calls.
                For Anthropic models, the "tool_calls" field on a message is only respected if content is a string.
                */
                if (currentContent.length > 0) {
                    let content = currentContent.reduce((acc, curr) => {
                        if (curr.type === ContentTypes.TEXT) {
                            return `${acc}${curr[ContentTypes.TEXT] || ''}\n`;
                        }
                        return acc;
                    }, '');
                    content =
                        `${content}\n${part[ContentTypes.TEXT] ?? part.text ?? ''}`.trim();
                    lastAIMessage = new AIMessage({ content });
                    formattedMessages.push(lastAIMessage);
                    currentContent = [];
                    continue;
                }
                // Create a new AIMessage with this text and prepare for tool calls
                lastAIMessage = new AIMessage({
                    content: part.text != null ? part.text : '',
                });
                formattedMessages.push(lastAIMessage);
            }
            else if (part.type === ContentTypes.TOOL_CALL) {
                // Skip malformed tool call entries without tool_call property
                if (part.tool_call == null) {
                    continue;
                }
                // Note: `tool_calls` list is defined when constructed by `AIMessage` class, and outputs should be excluded from it
                const { output, args: _args, ..._tool_call } = part.tool_call;
                // Skip invalid tool calls that have no name AND no output
                if (_tool_call.name == null ||
                    (_tool_call.name === '' && (output == null || output === ''))) {
                    continue;
                }
                if (!lastAIMessage) {
                    // "Heal" the payload by creating an AIMessage to precede the tool call
                    lastAIMessage = new AIMessage({ content: '' });
                    formattedMessages.push(lastAIMessage);
                }
                const tool_call = _tool_call;
                // TODO: investigate; args as dictionary may need to be providers-or-tool-specific
                let args = _args;
                try {
                    if (typeof _args === 'string') {
                        args = JSON.parse(_args);
                    }
                }
                catch {
                    if (typeof _args === 'string') {
                        args = { input: _args };
                    }
                }
                tool_call.args = args;
                if (!lastAIMessage.tool_calls) {
                    lastAIMessage.tool_calls = [];
                }
                lastAIMessage.tool_calls.push(tool_call);
                formattedMessages.push(new ToolMessage({
                    tool_call_id: tool_call.id ?? '',
                    name: tool_call.name,
                    content: output != null ? output : '',
                }));
            }
            else if (part.type === ContentTypes.THINK) {
                hasReasoning = true;
                continue;
            }
            else if (part.type === ContentTypes.ERROR ||
                part.type === ContentTypes.AGENT_UPDATE) {
                continue;
            }
            else {
                currentContent.push(part);
            }
        }
    }
    if (hasReasoning && currentContent.length > 0) {
        const content = currentContent
            .reduce((acc, curr) => {
            if (curr.type === ContentTypes.TEXT) {
                return `${acc}${curr[ContentTypes.TEXT] || ''}\n`;
            }
            return acc;
        }, '')
            .trim();
        if (content) {
            formattedMessages.push(new AIMessage({ content }));
        }
    }
    else if (currentContent.length > 0) {
        formattedMessages.push(new AIMessage({ content: currentContent }));
    }
    return formattedMessages;
}
/**
 * Labels all agent content for parallel patterns (fan-out/fan-in)
 * Groups consecutive content by agent and wraps with clear labels
 */
function labelAllAgentContent(contentParts, agentIdMap, agentNames) {
    const result = [];
    let currentAgentId;
    let agentContentBuffer = [];
    const flushAgentBuffer = () => {
        if (agentContentBuffer.length === 0) {
            return;
        }
        if (currentAgentId != null && currentAgentId !== '') {
            const agentName = (agentNames?.[currentAgentId] ?? '') || currentAgentId;
            const formattedParts = [];
            formattedParts.push(`--- ${agentName} ---`);
            for (const part of agentContentBuffer) {
                if (part.type === ContentTypes.THINK) {
                    const thinkContent = part.think || '';
                    if (thinkContent) {
                        formattedParts.push(`${agentName}: ${JSON.stringify({
                            type: 'think',
                            think: thinkContent,
                        })}`);
                    }
                }
                else if (part.type === ContentTypes.TEXT) {
                    const textContent = part.text ?? '';
                    if (textContent) {
                        formattedParts.push(`${agentName}: ${textContent}`);
                    }
                }
                else if (part.type === ContentTypes.TOOL_CALL) {
                    formattedParts.push(`${agentName}: ${JSON.stringify({
                        type: 'tool_call',
                        tool_call: part.tool_call,
                    })}`);
                }
            }
            formattedParts.push(`--- End of ${agentName} ---`);
            // Create a single text content part with all agent content
            result.push({
                type: ContentTypes.TEXT,
                text: formattedParts.join('\n\n'),
            });
        }
        else {
            // No agent ID, pass through as-is
            result.push(...agentContentBuffer);
        }
        agentContentBuffer = [];
    };
    for (let i = 0; i < contentParts.length; i++) {
        const part = contentParts[i];
        const agentId = agentIdMap[i];
        // If agent changed, flush previous buffer
        if (agentId !== currentAgentId && currentAgentId !== undefined) {
            flushAgentBuffer();
        }
        currentAgentId = agentId;
        agentContentBuffer.push(part);
    }
    // Flush any remaining content
    flushAgentBuffer();
    return result;
}
/**
 * Groups content parts by agent and formats them with agent labels
 * This preprocesses multi-agent content to prevent identity confusion
 *
 * @param contentParts - The content parts from a run
 * @param agentIdMap - Map of content part index to agent ID
 * @param agentNames - Optional map of agent ID to display name
 * @param options - Configuration options
 * @param options.labelNonTransferContent - If true, labels all agent transitions (for parallel patterns)
 * @returns Modified content parts with agent labels where appropriate
 */
const labelContentByAgent = (contentParts, agentIdMap, agentNames, options) => {
    if (!agentIdMap || Object.keys(agentIdMap).length === 0) {
        return contentParts;
    }
    // If labelNonTransferContent is true, use a different strategy for parallel patterns
    if (options?.labelNonTransferContent === true) {
        return labelAllAgentContent(contentParts, agentIdMap, agentNames);
    }
    const result = [];
    let currentAgentId;
    let agentContentBuffer = [];
    let transferToolCallIndex;
    let transferToolCallId;
    const flushAgentBuffer = () => {
        if (agentContentBuffer.length === 0) {
            return;
        }
        // If this is content from a transferred agent, format it specially
        if (currentAgentId != null &&
            currentAgentId !== '' &&
            transferToolCallIndex !== undefined) {
            const agentName = (agentNames?.[currentAgentId] ?? '') || currentAgentId;
            const formattedParts = [];
            formattedParts.push(`--- Transfer to ${agentName} ---`);
            for (const part of agentContentBuffer) {
                if (part.type === ContentTypes.THINK) {
                    formattedParts.push(`${agentName}: ${JSON.stringify({
                        type: 'think',
                        think: part.think,
                    })}`);
                }
                else if ('text' in part && part.type === ContentTypes.TEXT) {
                    const textContent = part.text ?? '';
                    if (textContent) {
                        formattedParts.push(`${agentName}: ${JSON.stringify({
                            type: 'text',
                            text: textContent,
                        })}`);
                    }
                }
                else if (part.type === ContentTypes.TOOL_CALL) {
                    formattedParts.push(`${agentName}: ${JSON.stringify({
                        type: 'tool_call',
                        tool_call: part.tool_call,
                    })}`);
                }
            }
            formattedParts.push(`--- End of ${agentName} response ---`);
            // Find the tool call that triggered this transfer and update its output
            if (transferToolCallIndex < result.length) {
                const transferToolCall = result[transferToolCallIndex];
                if (transferToolCall.type === ContentTypes.TOOL_CALL &&
                    transferToolCall.tool_call?.id === transferToolCallId) {
                    transferToolCall.tool_call.output = formattedParts.join('\n\n');
                }
            }
        }
        else {
            // Not from a transfer, add as-is
            result.push(...agentContentBuffer);
        }
        agentContentBuffer = [];
        transferToolCallIndex = undefined;
        transferToolCallId = undefined;
    };
    for (let i = 0; i < contentParts.length; i++) {
        const part = contentParts[i];
        const agentId = agentIdMap[i];
        // Check if this is a transfer tool call
        const isTransferTool = (part.type === ContentTypes.TOOL_CALL &&
            part.tool_call?.name?.startsWith('lc_transfer_to_')) ??
            false;
        // If agent changed, flush previous buffer
        if (agentId !== currentAgentId && currentAgentId !== undefined) {
            flushAgentBuffer();
        }
        currentAgentId = agentId;
        if (isTransferTool) {
            // Flush any existing buffer first
            flushAgentBuffer();
            // Add the transfer tool call to result
            result.push(part);
            // Mark that the next agent's content should be captured
            transferToolCallIndex = result.length - 1;
            transferToolCallId = part.tool_call?.id;
            currentAgentId = undefined; // Reset to capture the next agent
        }
        else {
            agentContentBuffer.push(part);
        }
    }
    flushAgentBuffer();
    return result;
};
/**
 * Formats an array of messages for LangChain, handling tool calls and creating ToolMessage instances.
 *
 * @param payload - The array of messages to format.
 * @param indexTokenCountMap - Optional map of message indices to token counts.
 * @param tools - Optional set of tool names that are allowed in the request.
 * @returns - Object containing formatted messages and updated indexTokenCountMap if provided.
 */
const formatAgentMessages = (payload, indexTokenCountMap, tools) => {
    const messages = [];
    // If indexTokenCountMap is provided, create a new map to track the updated indices
    const updatedIndexTokenCountMap = {};
    // Keep track of the mapping from original payload indices to result indices
    const indexMapping = {};
    // Process messages with tool conversion if tools set is provided
    for (let i = 0; i < payload.length; i++) {
        const message = payload[i];
        // Q: Store the current length of messages to track where this payload message starts in the result?
        // const startIndex = messages.length;
        if (typeof message.content === 'string') {
            message.content = [
                { type: ContentTypes.TEXT, [ContentTypes.TEXT]: message.content },
            ];
        }
        if (message.role !== 'assistant') {
            messages.push(formatMessage({
                message: message,
                langChain: true,
            }));
            // Update the index mapping for this message
            indexMapping[i] = [messages.length - 1];
            continue;
        }
        // For assistant messages, track the starting index before processing
        const startMessageIndex = messages.length;
        // If tools set is provided, we need to check if we need to convert tool messages to a string
        if (tools) {
            // First, check if this message contains tool calls
            let hasToolCalls = false;
            let hasInvalidTool = false;
            const content = message.content;
            if (content && Array.isArray(content)) {
                for (const part of content) {
                    if (part.type === ContentTypes.TOOL_CALL) {
                        hasToolCalls = true;
                        if (tools.size === 0) {
                            hasInvalidTool = true;
                            break;
                        }
                        // Protect against malformed tool call entries
                        if (part.tool_call == null ||
                            part.tool_call.name == null ||
                            part.tool_call.name === '') {
                            hasInvalidTool = true;
                            continue;
                        }
                        const toolName = part.tool_call.name;
                        if (!tools.has(toolName)) {
                            hasInvalidTool = true;
                        }
                    }
                }
            }
            // If this message has tool calls and at least one is invalid, we need to convert it
            if (hasToolCalls && hasInvalidTool) {
                // We need to collect all related messages (this message and any subsequent tool messages)
                const toolSequence = [];
                let sequenceEndIndex = i;
                // Process the current assistant message to get the AIMessage with tool calls
                const formattedMessages = formatAssistantMessage(message);
                toolSequence.push(...formattedMessages);
                // Look ahead for any subsequent assistant messages that might be part of this tool sequence
                let j = i + 1;
                while (j < payload.length && payload[j].role === 'assistant') {
                    // Check if this is a continuation of the tool sequence
                    let isToolResponse = false;
                    const content = payload[j].content;
                    if (content != null && Array.isArray(content)) {
                        for (const part of content) {
                            if (part.type === ContentTypes.TOOL_CALL) {
                                isToolResponse = true;
                                break;
                            }
                        }
                    }
                    if (isToolResponse) {
                        // This is part of the tool sequence, add it
                        const nextMessages = formatAssistantMessage(payload[j]);
                        toolSequence.push(...nextMessages);
                        sequenceEndIndex = j;
                        j++;
                    }
                    else {
                        // This is not part of the tool sequence, stop looking
                        break;
                    }
                }
                // Convert the sequence to a string
                const bufferString = getBufferString(toolSequence);
                messages.push(new AIMessage({ content: bufferString }));
                // Skip the messages we've already processed
                i = sequenceEndIndex;
                // Update the index mapping for this sequence
                const resultIndices = [messages.length - 1];
                for (let k = i; k >= i && k <= sequenceEndIndex; k++) {
                    indexMapping[k] = resultIndices;
                }
                continue;
            }
        }
        // Process the assistant message using the helper function
        const formattedMessages = formatAssistantMessage(message);
        messages.push(...formattedMessages);
        // Update the index mapping for this assistant message
        // Store all indices that were created from this original message
        const endMessageIndex = messages.length;
        const resultIndices = [];
        for (let j = startMessageIndex; j < endMessageIndex; j++) {
            resultIndices.push(j);
        }
        indexMapping[i] = resultIndices;
    }
    // Update the token count map if it was provided
    if (indexTokenCountMap) {
        for (let originalIndex = 0; originalIndex < payload.length; originalIndex++) {
            const resultIndices = indexMapping[originalIndex] || [];
            const tokenCount = indexTokenCountMap[originalIndex];
            if (tokenCount !== undefined) {
                if (resultIndices.length === 1) {
                    // Simple 1:1 mapping
                    updatedIndexTokenCountMap[resultIndices[0]] = tokenCount;
                }
                else if (resultIndices.length > 1) {
                    // If one message was split into multiple, distribute the token count
                    // This is a simplification - in reality, you might want a more sophisticated distribution
                    const countPerMessage = Math.floor(tokenCount / resultIndices.length);
                    resultIndices.forEach((resultIndex, idx) => {
                        if (idx === resultIndices.length - 1) {
                            // Give any remainder to the last message
                            updatedIndexTokenCountMap[resultIndex] =
                                tokenCount - countPerMessage * (resultIndices.length - 1);
                        }
                        else {
                            updatedIndexTokenCountMap[resultIndex] = countPerMessage;
                        }
                    });
                }
            }
        }
    }
    return {
        messages,
        indexTokenCountMap: indexTokenCountMap
            ? updatedIndexTokenCountMap
            : undefined,
    };
};
/**
 * Adds a value at key 0 for system messages and shifts all key indices by one in an indexTokenCountMap.
 * This is useful when adding a system message at the beginning of a conversation.
 *
 * @param indexTokenCountMap - The original map of message indices to token counts
 * @param instructionsTokenCount - The token count for the system message to add at index 0
 * @returns A new map with the system message at index 0 and all other indices shifted by 1
 */
function shiftIndexTokenCountMap(indexTokenCountMap, instructionsTokenCount) {
    // Create a new map to avoid modifying the original
    const shiftedMap = {};
    shiftedMap[0] = instructionsTokenCount;
    // Shift all existing indices by 1
    for (const [indexStr, tokenCount] of Object.entries(indexTokenCountMap)) {
        const index = Number(indexStr);
        shiftedMap[index + 1] = tokenCount;
    }
    return shiftedMap;
}
/**
 * Ensures compatibility when switching from a non-thinking agent to a thinking-enabled agent.
 * Converts AI messages with tool calls (that lack thinking/reasoning blocks) into buffer strings,
 * avoiding the thinking block signature requirement.
 *
 * Recognizes the following as valid thinking/reasoning blocks:
 * - ContentTypes.THINKING (Anthropic)
 * - ContentTypes.REASONING_CONTENT (Bedrock)
 * - ContentTypes.REASONING (VertexAI / Google)
 * - 'redacted_thinking'
 *
 * @param messages - Array of messages to process
 * @param provider - The provider being used (unused but kept for future compatibility)
 * @returns The messages array with tool sequences converted to buffer strings if necessary
 */
function ensureThinkingBlockInMessages(messages, _provider) {
    const result = [];
    let i = 0;
    while (i < messages.length) {
        const msg = messages[i];
        const isAI = msg instanceof AIMessage || msg instanceof AIMessageChunk;
        if (!isAI) {
            result.push(msg);
            i++;
            continue;
        }
        const aiMsg = msg;
        const hasToolCalls = aiMsg.tool_calls && aiMsg.tool_calls.length > 0;
        const contentIsArray = Array.isArray(aiMsg.content);
        // Check if the message has tool calls or tool_use content
        let hasToolUse = hasToolCalls ?? false;
        let firstContentType;
        if (contentIsArray && aiMsg.content.length > 0) {
            const content = aiMsg.content;
            firstContentType = content[0]?.type;
            hasToolUse =
                hasToolUse ||
                    content.some((c) => typeof c === 'object' && c.type === 'tool_use');
        }
        // If message has tool use but no thinking block, convert to buffer string
        if (hasToolUse &&
            firstContentType !== ContentTypes.THINKING &&
            firstContentType !== ContentTypes.REASONING_CONTENT &&
            firstContentType !== ContentTypes.REASONING &&
            firstContentType !== 'redacted_thinking') {
            // Collect the AI message and any following tool messages
            const toolSequence = [msg];
            let j = i + 1;
            // Look ahead for tool messages that belong to this AI message
            while (j < messages.length && messages[j] instanceof ToolMessage) {
                toolSequence.push(messages[j]);
                j++;
            }
            // Convert the sequence to a buffer string and wrap in a HumanMessage
            // This avoids the thinking block requirement which only applies to AI messages
            const bufferString = getBufferString(toolSequence);
            result.push(new HumanMessage({
                content: `[Previous agent context]\n${bufferString}`,
            }));
            // Skip the messages we've processed
            i = j;
        }
        else {
            // Keep the message as is
            result.push(msg);
            i++;
        }
    }
    return result;
}

export { ensureThinkingBlockInMessages, formatAgentMessages, formatFromLangChain, formatLangChainMessages, formatMediaMessage, formatMessage, labelContentByAgent, shiftIndexTokenCountMap };
//# sourceMappingURL=format.mjs.map
